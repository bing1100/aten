{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4db90df",
   "metadata": {},
   "source": [
    "# Classification using HF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd3fc045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhux/anaconda3/envs/aten/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c98414",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "1. load the data\n",
    "2. encode the labels\n",
    "3. Create train, validation, and test sets\n",
    "4. tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1743608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "classification_df = pd.read_csv(\"./data/classification_data.csv\")\n",
    "\n",
    "# Encode Labels\n",
    "label_encoder = LabelEncoder()\n",
    "classification_df['label_encoded'] = label_encoder.fit_transform(classification_df['label'])\n",
    "\n",
    "# Create Test Set\n",
    "tv_texts, test_texts, tv_labels, test_labels = train_test_split(\n",
    "    classification_df['text'].tolist(),\n",
    "    classification_df['label_encoded'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create train and validation set\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    tv_texts,\n",
    "    tv_labels,\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Tokenize the data\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c036b4a6",
   "metadata": {},
   "source": [
    "## Define data class\n",
    "\n",
    "1. define the data class - add any data augmentations here\n",
    "2. transform to dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c73fb8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ClassificationDataset(train_encodings, train_labels)\n",
    "val_dataset = ClassificationDataset(val_encodings, val_labels)\n",
    "test_dataset = ClassificationDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872a9fe2",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "1. define metrics and objectives\n",
    "2. initialize the model, training arguments, and trainer\n",
    "3. define and run hpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f206df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2025-05-09 15:59:33,931] A new study created in memory with name: no-name-f3322aa9-e09a-493f-9ac7-5707adea13aa\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 00:04, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.018800</td>\n",
       "      <td>0.889461</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.776300</td>\n",
       "      <td>0.593663</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.495100</td>\n",
       "      <td>0.345062</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.299600</td>\n",
       "      <td>0.194786</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.175200</td>\n",
       "      <td>0.113124</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.107900</td>\n",
       "      <td>0.070366</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>0.048810</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>0.037562</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.043100</td>\n",
       "      <td>0.031127</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>0.027147</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.024561</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.022836</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.021716</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>0.021093</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.020880</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 15:59:39,352] Trial 0 finished with values: [0.020879525691270828, 1.0] and parameters: {'learning_rate': 1.5149715323249793e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 15, 'weight_decay': 0.006038666974230823}.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:03, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.085744</td>\n",
       "      <td>0.243107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.087100</td>\n",
       "      <td>1.077743</td>\n",
       "      <td>0.469300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.087100</td>\n",
       "      <td>1.070061</td>\n",
       "      <td>0.646386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.076500</td>\n",
       "      <td>1.062522</td>\n",
       "      <td>0.857596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.076500</td>\n",
       "      <td>1.055166</td>\n",
       "      <td>0.857596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.057100</td>\n",
       "      <td>1.048038</td>\n",
       "      <td>0.857596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.057100</td>\n",
       "      <td>1.041442</td>\n",
       "      <td>0.857596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.048200</td>\n",
       "      <td>1.035372</td>\n",
       "      <td>0.857596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.048200</td>\n",
       "      <td>1.029739</td>\n",
       "      <td>0.848054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.034700</td>\n",
       "      <td>1.024778</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.034700</td>\n",
       "      <td>1.020571</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.025500</td>\n",
       "      <td>1.017085</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.025500</td>\n",
       "      <td>1.014495</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.013400</td>\n",
       "      <td>1.012951</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.013400</td>\n",
       "      <td>1.012343</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 15:59:43,141] Trial 1 finished with values: [1.012343168258667, 0.91215559925138] and parameters: {'learning_rate': 1.5046201713508165e-06, 'per_device_train_batch_size': 32, 'num_train_epochs': 15, 'weight_decay': 0.012162159035407318}.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.061231</td>\n",
       "      <td>0.857596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.029002</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.996673</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.965109</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.024900</td>\n",
       "      <td>0.934735</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.024900</td>\n",
       "      <td>0.906861</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.024900</td>\n",
       "      <td>0.882748</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.024900</td>\n",
       "      <td>0.863873</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.024900</td>\n",
       "      <td>0.851438</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.884700</td>\n",
       "      <td>0.845991</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 15:59:45,354] Trial 2 finished with values: [0.845991313457489, 0.91215559925138] and parameters: {'learning_rate': 1.217099314629715e-05, 'per_device_train_batch_size': 128, 'num_train_epochs': 10, 'weight_decay': 0.018658280236505335}.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:02, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.091556</td>\n",
       "      <td>0.174988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.089054</td>\n",
       "      <td>0.243107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.086783</td>\n",
       "      <td>0.243107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.084654</td>\n",
       "      <td>0.243107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.088000</td>\n",
       "      <td>1.082685</td>\n",
       "      <td>0.243107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.088000</td>\n",
       "      <td>1.080860</td>\n",
       "      <td>0.360723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.088000</td>\n",
       "      <td>1.079187</td>\n",
       "      <td>0.429496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.088000</td>\n",
       "      <td>1.077698</td>\n",
       "      <td>0.429496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.088000</td>\n",
       "      <td>1.076361</td>\n",
       "      <td>0.534871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.077000</td>\n",
       "      <td>1.075199</td>\n",
       "      <td>0.615310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.077000</td>\n",
       "      <td>1.074233</td>\n",
       "      <td>0.615310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.077000</td>\n",
       "      <td>1.073469</td>\n",
       "      <td>0.615310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.077000</td>\n",
       "      <td>1.072907</td>\n",
       "      <td>0.615310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.077000</td>\n",
       "      <td>1.072571</td>\n",
       "      <td>0.615310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.074000</td>\n",
       "      <td>1.072415</td>\n",
       "      <td>0.615310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 15:59:48,527] Trial 3 finished with values: [1.072414517402649, 0.6153099830421596] and parameters: {'learning_rate': 1.0505442838742028e-06, 'per_device_train_batch_size': 128, 'num_train_epochs': 15, 'weight_decay': 0.006636830916584974}.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:01, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.725104</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.776400</td>\n",
       "      <td>0.355478</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.776400</td>\n",
       "      <td>0.190220</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.241300</td>\n",
       "      <td>0.126173</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.241300</td>\n",
       "      <td>0.106691</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 15:59:50,163] Trial 4 finished with values: [0.10669145733118057, 1.0] and parameters: {'learning_rate': 5.560205083581748e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 5, 'weight_decay': 0.011448069009677628}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BestRun(run_id='0', objective=[0.020879525691270828, 1.0], hyperparameters={'learning_rate': 1.5149715323249793e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 15, 'weight_decay': 0.006038666974230823}, run_summary=None)]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Define metrics and objectives\n",
    "metric = evaluate.combine([\"f1\"])\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "def compute_objective(metric):\n",
    "    return metric[\"eval_loss\"], metric[\"eval_f1\"]\n",
    "\n",
    "# define model\n",
    "def model_init(trial):\n",
    "    return DistilBertForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\", \n",
    "        num_labels=len(label_encoder.classes_)\n",
    "    )\n",
    "\n",
    "# define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/tmp/results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"/tmp/logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    eval_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# define trainer\n",
    "hpt_trainer = Trainer(\n",
    "    model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=val_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    model_init=model_init,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# define and run hyperparameter training\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64, 128]),\n",
    "        \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [5, 10, 15]),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-3, 3e-2)\n",
    "    }\n",
    "\n",
    "best_trials = hpt_trainer.hyperparameter_search(\n",
    "    direction=[\"minimize\", \"maximize\"],\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=5, # this can be increased\n",
    "    compute_objective=compute_objective\n",
    ")\n",
    "\n",
    "print(best_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95976b70",
   "metadata": {},
   "source": [
    "## Fine-tune the model with best HPs\n",
    "\n",
    "1. define validation metrics\n",
    "2. define training args with optimized HPs\n",
    "3. fine-tune the model with validation on overfitting\n",
    "4. evaluate the model on test set (note that we achieve 100% accuracy during training, validation, and test)\n",
    "5. save the model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfb77678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 00:10, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.326400</td>\n",
       "      <td>0.193874</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.026333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.017400</td>\n",
       "      <td>0.012565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.008117</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.005929</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.003807</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.003239</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.002027</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define validation metrics \n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    metrics = {}\n",
    "    metrics.update(accuracy.compute(predictions=predictions, references=labels))\n",
    "    metrics.update(precision.compute(predictions=predictions, references=labels, average='weighted'))\n",
    "    metrics.update(recall.compute(predictions=predictions, references=labels, average='weighted'))\n",
    "    metrics.update(f1.compute(predictions=predictions, references=labels, average='weighted'))\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# setup training args with HPs\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/tmp/results\",\n",
    "    learning_rate=best_trials[0].hyperparameters['learning_rate'],\n",
    "    per_device_train_batch_size=best_trials[0].hyperparameters['per_device_train_batch_size'],\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=best_trials[0].hyperparameters['num_train_epochs'],\n",
    "    weight_decay=best_trials[0].hyperparameters['weight_decay'],\n",
    "    logging_dir=\"/tmp/logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    eval_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(label_encoder.classes_))\n",
    "model.to('cuda')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# fine-tune and evaluate on the test dataset\n",
    "losses = trainer.train()\n",
    "\n",
    "trainer.evaluate(test_dataset)\n",
    "\n",
    "# save the model\n",
    "model.save_pretrained(\"./model/bioclassification-distilbert-base-uncased\", from_pt=True)\n",
    "\n",
    "# dump the training args\n",
    "import pickle\n",
    "with open('./model/training_args.pkl', 'wb') as file: \n",
    "    pickle.dump(training_args, file) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577aa8aa",
   "metadata": {},
   "source": [
    "## Online Model Inference\n",
    "\n",
    "1. load fine-tuned model and hyperparameter tuned arguments\n",
    "2. define batch examples\n",
    "3. tokenize and create dataset\n",
    "4. feed into inference \n",
    "5. collect and map preds to human readable output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87e42d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DrugZ caused severe rashes in some participants.': 'Adverse Effect', 'Increased liver enzymes were noted post-treatment with DrugA.': 'Adverse Effect', 'The study excluded patients with pre-existing conditions.': 'Neutral Observation', 'No significant side effects were observed during the trial.': 'Positive Outcome', 'The treatment resulted in full remission for the majority of patients.': 'Positive Outcome'}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./model/training_args.pkl', 'rb') as file: \n",
    "    training_args = pickle.load(file) \n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"./model/bioclassification-distilbert-base-uncased\", num_labels=len(label_encoder.classes_))\n",
    "model.to('cuda')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Batch examples - labels are optional\n",
    "examples = {\n",
    "    \"DrugZ caused severe rashes in some participants.\": 0,\n",
    "    \"Increased liver enzymes were noted post-treatment with DrugA.\": 0,\n",
    "    \"The study excluded patients with pre-existing conditions.\": 1,\n",
    "    \"No significant side effects were observed during the trial.\" : 2,\n",
    "    \"The treatment resulted in full remission for the majority of patients.\": 2\n",
    "}\n",
    "\n",
    "# tokenize and create dataset\n",
    "example_encoding = tokenizer([k for k,_ in examples.items()], truncation=True, padding=True)\n",
    "example_labels = [v for _,v in examples.items()]\n",
    "\n",
    "example_dataset = ClassificationDataset(example_encoding, example_labels)\n",
    "\n",
    "# feed into inference\n",
    "preds = trainer.predict(example_dataset)\n",
    "\n",
    "# collect and map preds to human readable output\n",
    "preds = np.argmax(preds.predictions, axis=1)\n",
    "mapping = {\n",
    "    0: \"Adverse Effect\",\n",
    "    1: \"Neutral Observation\",\n",
    "    2: \"Positive Outcome\"\n",
    "}\n",
    "print({k:mapping[v] for k, v in zip(examples.keys(), preds)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aten",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
