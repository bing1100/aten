{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd3fc045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhux/anaconda3/envs/aten/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c98414",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "1. load the data\n",
    "2. encode the labels\n",
    "3. Create train, validation, and test sets\n",
    "4. tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1743608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "classification_df = pd.read_csv(\"./data/classification_data.csv\")\n",
    "\n",
    "# Encode Labels\n",
    "label_encoder = LabelEncoder()\n",
    "classification_df['label_encoded'] = label_encoder.fit_transform(classification_df['label'])\n",
    "\n",
    "# Create Test Set\n",
    "tv_texts, test_texts, tv_labels, test_labels = train_test_split(\n",
    "    classification_df['text'].tolist(),\n",
    "    classification_df['label_encoded'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create train and validation set\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    tv_texts,\n",
    "    tv_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Tokenize the data\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c036b4a6",
   "metadata": {},
   "source": [
    "# Define data class\n",
    "\n",
    "1. define the data class - add any data augmentations here\n",
    "2. transform to dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c73fb8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ClassificationDataset(train_encodings, train_labels)\n",
    "val_dataset = ClassificationDataset(val_encodings, val_labels)\n",
    "test_dataset = ClassificationDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872a9fe2",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "1. define metrics and objectives\n",
    "2. initialize the model, training arguments, and trainer\n",
    "3. define and run hpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f206df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2025-05-09 10:32:59,607] A new study created in memory with name: no-name-ab72c096-8350-4b62-a6da-d4e18e223119\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:01, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.067300</td>\n",
       "      <td>1.023134</td>\n",
       "      <td>0.788441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.995900</td>\n",
       "      <td>0.942992</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.916200</td>\n",
       "      <td>0.869004</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.845200</td>\n",
       "      <td>0.818150</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.810100</td>\n",
       "      <td>0.798494</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 10:33:02,096] Trial 0 finished with values: [0.7984938621520996, 0.91215559925138] and parameters: {'learning_rate': 6.710901574953645e-06, 'per_device_train_batch_size': 16, 'num_train_epochs': 5, 'weight_decay': 0.02427021108221012}.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:03, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.076956</td>\n",
       "      <td>0.374879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>1.059428</td>\n",
       "      <td>0.857596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>1.042115</td>\n",
       "      <td>0.857596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.050100</td>\n",
       "      <td>1.024676</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.050100</td>\n",
       "      <td>1.006104</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.008600</td>\n",
       "      <td>0.986866</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.008600</td>\n",
       "      <td>0.968363</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.976400</td>\n",
       "      <td>0.950932</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.976400</td>\n",
       "      <td>0.934090</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.939600</td>\n",
       "      <td>0.918572</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.939600</td>\n",
       "      <td>0.905368</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.913700</td>\n",
       "      <td>0.894514</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.913700</td>\n",
       "      <td>0.886409</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.887800</td>\n",
       "      <td>0.881528</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.887800</td>\n",
       "      <td>0.879654</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 10:33:06,100] Trial 1 finished with values: [0.8796542882919312, 0.91215559925138] and parameters: {'learning_rate': 3.0463861314729984e-06, 'per_device_train_batch_size': 32, 'num_train_epochs': 15, 'weight_decay': 0.027378892334551277}.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:03, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.075900</td>\n",
       "      <td>1.044932</td>\n",
       "      <td>0.718109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.027400</td>\n",
       "      <td>0.986632</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.967500</td>\n",
       "      <td>0.921472</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.890600</td>\n",
       "      <td>0.850396</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.823300</td>\n",
       "      <td>0.783146</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.765700</td>\n",
       "      <td>0.723680</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.713500</td>\n",
       "      <td>0.678250</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.670900</td>\n",
       "      <td>0.645373</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.656600</td>\n",
       "      <td>0.624603</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.632100</td>\n",
       "      <td>0.616997</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 10:33:09,736] Trial 2 finished with values: [0.6169969439506531, 1.0] and parameters: {'learning_rate': 4.59965584961873e-06, 'per_device_train_batch_size': 16, 'num_train_epochs': 10, 'weight_decay': 0.0031407009974995714}.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.091010</td>\n",
       "      <td>0.243107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.088496</td>\n",
       "      <td>0.243107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.086659</td>\n",
       "      <td>0.243107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.085507</td>\n",
       "      <td>0.243107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.088000</td>\n",
       "      <td>1.084991</td>\n",
       "      <td>0.243107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 10:33:11,152] Trial 3 finished with values: [1.0849907398223877, 0.2431067431067431] and parameters: {'learning_rate': 1.295339521918524e-06, 'per_device_train_batch_size': 128, 'num_train_epochs': 5, 'weight_decay': 0.0012290607859959113}.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.819031</td>\n",
       "      <td>0.912156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.510768</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.293046</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.693800</td>\n",
       "      <td>0.174852</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.693800</td>\n",
       "      <td>0.111480</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.693800</td>\n",
       "      <td>0.076835</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.149300</td>\n",
       "      <td>0.058232</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.149300</td>\n",
       "      <td>0.048021</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.149300</td>\n",
       "      <td>0.042822</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.056800</td>\n",
       "      <td>0.040990</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 10:33:13,883] Trial 4 finished with values: [0.040989574044942856, 1.0] and parameters: {'learning_rate': 6.0241314473928826e-05, 'per_device_train_batch_size': 64, 'num_train_epochs': 10, 'weight_decay': 0.02320569314066657}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BestRun(run_id='4', objective=[0.040989574044942856, 1.0], hyperparameters={'learning_rate': 6.0241314473928826e-05, 'per_device_train_batch_size': 64, 'num_train_epochs': 10, 'weight_decay': 0.02320569314066657}, run_summary=None)]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Define metrics and objectives\n",
    "metric = evaluate.combine([\"f1\"])\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "def compute_objective(metric):\n",
    "    return metric[\"eval_loss\"], metric[\"eval_f1\"]\n",
    "\n",
    "# define model\n",
    "def model_init(trial):\n",
    "    return DistilBertForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\", \n",
    "        num_labels=len(label_encoder.classes_)\n",
    "    )\n",
    "\n",
    "# define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/tmp/results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"/tmp/logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    eval_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# define trainer\n",
    "hpt_trainer = Trainer(\n",
    "    model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=val_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    model_init=model_init,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# define and run hyperparameter training\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64, 128]),\n",
    "        \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [5, 10, 15]),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-3, 3e-2)\n",
    "    }\n",
    "\n",
    "best_trials = hpt_trainer.hyperparameter_search(\n",
    "    direction=[\"minimize\", \"maximize\"],\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=5,\n",
    "    compute_objective=compute_objective\n",
    ")\n",
    "\n",
    "print(best_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95976b70",
   "metadata": {},
   "source": [
    "# Fine-tune the model with best HPs\n",
    "\n",
    "1. define validation metrics\n",
    "2. define training args with optimized HPs\n",
    "3. fine-tune the model with validation on overfitting\n",
    "4. evaluate the model on test set\n",
    "5. save the model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb77678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:02, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.807600</td>\n",
       "      <td>0.386372</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.202400</td>\n",
       "      <td>0.055676</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.037300</td>\n",
       "      <td>0.016093</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.008256</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.005632</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.004461</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.003506</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.003328</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define validation metrics \n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    metrics = {}\n",
    "    metrics.update(accuracy.compute(predictions=predictions, references=labels))\n",
    "    metrics.update(precision.compute(predictions=predictions, references=labels, average='weighted'))\n",
    "    metrics.update(recall.compute(predictions=predictions, references=labels, average='weighted'))\n",
    "    metrics.update(f1.compute(predictions=predictions, references=labels, average='weighted'))\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# setup training args with HPs\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/tmp/results\",\n",
    "    learning_rate=best_trials[0].hyperparameters['learning_rate'],\n",
    "    per_device_train_batch_size=best_trials[0].hyperparameters['per_device_train_batch_size'],\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=best_trials[0].hyperparameters['num_train_epochs'],\n",
    "    weight_decay=best_trials[0].hyperparameters['weight_decay'],\n",
    "    logging_dir=\"/tmp/logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    eval_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(label_encoder.classes_))\n",
    "model.to('cuda')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# fine-tune and evaluate on the test dataset\n",
    "losses = trainer.train()\n",
    "\n",
    "trainer.evaluate(test_dataset)\n",
    "\n",
    "# save the model\n",
    "model.save_pretrained(\"./model/bioclassification-distilbert-base-uncased\", from_pt=True)\n",
    "\n",
    "# dump the training args\n",
    "import pickle\n",
    "with open('./model/training_args.pkl', 'wb') as file: \n",
    "    pickle.dump(training_args, file) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577aa8aa",
   "metadata": {},
   "source": [
    "# Online Model Inference\n",
    "\n",
    "1. load fine-tuned model and hyperparameter tuned arguments\n",
    "2. define batch examples\n",
    "3. tokenize and create dataset\n",
    "4. feed into inference \n",
    "5. collect and map preds to human readable output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87e42d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./model/training_args.pkl', 'rb') as file: \n",
    "    training_args = pickle.load(file) \n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"./model/bioclassification-distilbert-base-uncased\", num_labels=len(label_encoder.classes_))\n",
    "model.to('cuda')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ba32c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DrugZ caused severe rashes in some participants.': 'Adverse Effect'}\n"
     ]
    }
   ],
   "source": [
    "# Batch examples - labels are optional\n",
    "examples = {\n",
    "    \"DrugZ caused severe rashes in some participants.\": 0\n",
    "}\n",
    "\n",
    "# tokenize and create dataset\n",
    "example_encoding = tokenizer([k for k,_ in examples.items()], truncation=True, padding=True)\n",
    "example_labels = [v for _,v in examples.items()]\n",
    "\n",
    "example_dataset = ClassificationDataset(example_encoding, example_labels)\n",
    "\n",
    "# feed into inference\n",
    "preds = trainer.predict(example_dataset)\n",
    "\n",
    "# collect and map preds to human readable output\n",
    "preds = np.argmax(preds.predictions, axis=1)\n",
    "mapping = {\n",
    "    0: \"Adverse Effect\",\n",
    "    1: \"Neutral Observation\",\n",
    "    2: \"Positive Outcome\"\n",
    "}\n",
    "print({k:mapping[v] for k, v in zip(examples.keys(), preds)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aten",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
